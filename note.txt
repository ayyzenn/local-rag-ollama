Minimal RAG with Local Ollama

Letâ€™s build a local RAG prototype using:

> Ollama for LLM (llama3)
> LangChain or pure Python
> ChromaDB as the vector store
